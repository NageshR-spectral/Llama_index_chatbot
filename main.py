from cassandra.auth import PlainTextAuthProvider
from cassandra.cluster import Cluster
from llama_index import ServiceContext
from llama_index import set_global_service_context
from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.embeddings import GradientEmbedding
from llama_index.llms import GradientBaseModelLLM
import json
import config

#  = os.getenv("OPENAI_API_KEY")
token = config.GRADIENT_ACCESS_TOKEN
workspace_id = config.GRADIENT_WORKSPACE_ID

class Main:
    def __init__(self,folder_path):
        self.folder_path = folder_path
        # This secure connect bundle is autogenerated when you donwload your SCB,
        # if yours is different update the file name below
        cloud_config= {
        'secure_connect_bundle': 'credentials/secure-connect-nagesh.zip'
        }

        # This token json file is autogenerated when you donwload your token,
        # if yours is different update the file name below
        with open("credentials/nagesh.rathod@spectraltech.ai-token.json") as f:
            secrets = json.load(f)

        CLIENT_ID = secrets["clientId"]
        CLIENT_SECRET = secrets["secret"]

        auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
        cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
        session = cluster.connect()

        row = session.execute("select release_version from system.local").one()
        if row:
            print(row[0])
        else:
            print("An error occurred.")

    def create_llm(self):

        llm = GradientBaseModelLLM(
            base_model_slug="llama2-7b-chat",
            max_tokens=400,
        )
        return llm

    def create_model(self):
        embed_model = GradientEmbedding(
            gradient_access_token = token,
            gradient_workspace_id = workspace_id,
            gradient_model_slug="bge-large",
        )
        return embed_model

    def create_query_engine(self,llm,embed_model):
        service_context = ServiceContext.from_defaults(
            llm = llm,
            embed_model = embed_model,
            chunk_size=256,
        )

        set_global_service_context(service_context)

        documents = SimpleDirectoryReader(self.folder_path).load_data()
        print(f"Loaded {len(documents)} document(s).")

        index = VectorStoreIndex.from_documents(documents,
                                                service_context=service_context)
        query_engine = index.as_query_engine()
        
        return query_engine
    